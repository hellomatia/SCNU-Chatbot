import { ChatOpenAI } from 'langchain/chat_models/openai';
import { PineconeStore } from 'langchain/vectorstores/pinecone';
import { ConversationalRetrievalQAChain } from 'langchain/chains';

const CONDENSE_TEMPLATE = `ë‹¤ìŒ ëŒ€í™”ì™€ í›„ì† ì§ˆë¬¸ì´ ì£¼ì–´ì¡Œì„ ë•Œ í›„ì† ì§ˆë¬¸ì— ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¾¸ì–´ ë³´ì„¸ìš”.

Chat History:
{chat_history}
Follow Up Input: {question}
Standalone question:`;

const QA_TEMPLATE = `ë‹¹ì‹ ì€ ìˆœì²œëŒ€í•™êµ í•™ì‚¬ì— ê´€ë ¨ëœ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì¹œì ˆí•œ í•™ì‚¬ ì•ˆë‚´ ì±—ë´‡ í–¥ë¦¼ì´ðŸŽ“ ìž…ë‹ˆë‹¤.
ì´ëª¨ì§€ðŸ˜Šë¥¼ ë¬´ì¡°ê±´ ì¶”ê°€í•˜ì—¬ ë‹µí•´ì£¼ì„¸ìš”.
ì²« ë¬¸ìž¥ì€ ë§¤ìš° ì¹œì ˆí•œ ìƒë‹´ê°€ ì²˜ëŸ¼ ì‚¬ìš©ìžì˜ ë‹µë³€ì— ê³µê°í•˜ë©´ì„œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.ðŸ“š
ë§ˆì§€ë§‰ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.
ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ê³ , ì—†ëŠ” ë‹µì„ ì–µì§€ë¡œ ì§€ì–´ë‚´ë ¤ê³  í•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.
ë‹µì„ ëª¨ë¥¼ ê²½ìš°, ì•ˆë‚´ë²ˆí˜¸(061-000-1234)ë¡œ ìžì„¸ížˆ ë¬¸ì˜í•˜ë„ë¡ ìœ ë„í•˜ì„¸ìš”.


{context}

ì§ˆë¬¸ : {question}
Helpful answer in markdown:`;

export const makeChain = (vectorstore: PineconeStore) => {
  const model = new ChatOpenAI({
    temperature: 0.2, // increase temepreature to get more creative answers
    modelName: 'gpt-3.5-turbo', //change this to gpt-4 if you have access
  });

  const chain = ConversationalRetrievalQAChain.fromLLM(
    model,
    vectorstore.asRetriever(),
    {
      qaTemplate: QA_TEMPLATE,
      questionGeneratorTemplate: CONDENSE_TEMPLATE,
      returnSourceDocuments: true, //The number of source documents returned is 4 by default
    },
  );
  return chain;
};
