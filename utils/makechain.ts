import { ChatOpenAI } from 'langchain/chat_models/openai';
import { PineconeStore } from 'langchain/vectorstores/pinecone';
import { ConversationalRetrievalQAChain } from 'langchain/chains';

const CONDENSE_TEMPLATE = `[ì €ë²ˆ ëŒ€í™” ë‚´ìš©]ê³¼ [í›„ì† ì§ˆë¬¸]ì´ ì£¼ì–´ì¡Œì„ ë•Œ [í›„ì† ì§ˆë¬¸]ì„ [ì €ë²ˆ ëŒ€í™” ë‚´ìš©ì„ í¬í•¨í•œ í›„ì† ì§ˆë¬¸]ìœ¼ë¡œ ë°”ê¾¸ì–´ ë³´ì„¸ìš”.
...
[ì €ë²ˆ ëŒ€í™” ë‚´ìš©] : {chat_history}
...
[í›„ì† ì§ˆë¬¸] : {question}
...
[ì €ë²ˆ ëŒ€í™” ë‚´ìš©ì„ í¬í•¨í•œ í›„ì† ì§ˆë¬¸] :`;

const QA_TEMPLATE = `ë‹¹ì‹ ì€ ìˆœì²œëŒ€í•™êµ í•™ì‚¬ì— ê´€ë ¨ëœ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì¹œì ˆí•œ í•™ì‚¬ ì•ˆë‚´ ì±—ë´‡ í–¥ë¦¼ì´ðŸŽ“ ìž…ë‹ˆë‹¤.
[ë‹µë³€]ë¬¸ìž¥ì— ë§žëŠ” ì´ëª¨ì§€ë¥¼ ë¬´ì¡°ê±´ ì¶”ê°€í•˜ì—¬ ë‹µí•´ì£¼ì„¸ìš”!!!
ì²« ë¬¸ìž¥ì€ ë§¤ìš° ì¹œì ˆí•œ ìƒë‹´ê°€ ì²˜ëŸ¼ ì‚¬ìš©ìžì˜ [ì§ˆë¬¸]ì„ ë°˜ë³µí•˜ì—¬ ë‹¹ì‹ ì´ ì´í•´í–ˆëŠ”ì§€ [ë‹µë³€]í•´ì£¼ì„¸ìš”.ðŸ“š
ì•„ëž˜ [ë‚´ìš©]ì„ ì°¸ê³ í•˜ê³ , ë‹¤ì‹œí•œë²ˆ ìƒê°í•˜ì—¬ ì§ˆë¬¸ì— ì§§ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ì•ˆë‚´í•˜ê³ , ì—†ëŠ” ë‹µë³€ì„ ì–µì§€ë¡œ ì§€ì–´ë‚´ë ¤ê³  í•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.
ë‹µë³€ì— ìˆœì„œë¥¼ ì‚¬ìš©í•  ê²½ìš° ìˆœì„œ ìˆ«ìž. ë°©ì‹ ë§ê³ , (ìˆœì„œ ìˆ«ìž)ë¡œ ë‹µë³€ í•´ì£¼ì„¸ìš”.
ë‹µë³€ì„ ëª¨ë¥¼ ê²½ìš°, ì•ˆë‚´ë²ˆí˜¸(061-000-1234)ë¡œ ìžì„¸ížˆ ë¬¸ì˜í•˜ë„ë¡ ìœ ë„í•˜ì„¸ìš”.
...
[ë‚´ìš©] : {context}
...
[ì§ˆë¬¸] : {question}
...
ë„ì›€ì´ ë˜ëŠ” [ë‹µë³€] : `;

export const makeChain = (vectorstore: PineconeStore) => {
  const model = new ChatOpenAI({
    temperature: 0.2, // increase temepreature to get more creative answers
    modelName: 'gpt-3.5-turbo', //change this to gpt-4 if you have access
  });

  const chain = ConversationalRetrievalQAChain.fromLLM(
    model,
    vectorstore.asRetriever(),
    {
      qaTemplate: QA_TEMPLATE,
      questionGeneratorTemplate: CONDENSE_TEMPLATE,
      returnSourceDocuments: true, //The number of source documents returned is 4 by default
    },
  );
  return chain;
};
